%\RequirePackage[l2tabu, orthodox]{nag}

\documentclass[11pt,oneside]{report}
\usepackage[a4paper, margin=1.5in]{geometry}
%\usepackage{fullpage}
\usepackage{url}
\usepackage{graphicx}
\usepackage{harvard}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{bashful}
\usepackage{microtype}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage[font={small,it}]{caption}

\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{\leftmark}
\cfoot{}

\citationmode{abbr}
\renewcommand{\baselinestretch}{1.5}

\newcommand{\varusecase}{\textbf{Use case}}
\newcommand{\vardescription}{Description}
\newcommand{\varactor}{Actor}
\newcommand{\varentry}{Entry Condition}
\newcommand{\varflow}{Flow of Events}
\newcommand{\varaltflow}{Alternative Flow}
\newcommand{\varexit}{Exit Condition}

%\renewcommand{\familydefault}{\sfdefault}


\graphicspath{{images/}}

\title{An Open Source Computer Vision Library for Leapmotion}
\author{Daniel Hamilton 10026535,\\Computer Science,\\University of the West of England.}
	
\bash
texcount -merge -sum -1 -opt=TCOpt.txt report.tex
\END

\begin{document}
	\maketitle
	\tableofcontents

	
	Word Count: \bashStdout.
	\begin{abstract}
	
	\end{abstract}	
	%----------------------------------------- Chapter 1 -----------------------------------------%
	\chapter{Introduction}\label{chap:introduction}
		\section{Computer Vision: An Intellectual Frontier}
			\subsection{What is Computer Vision?}		
				
				It is very hard for humans to know what their own biological vision really entails, and how difficult it is to reproduce on a computer.
				Information from the eyes is divided into multiple channels, each streaming different kinds of information to the brain.
				The brain then subconsciously groups and identifies the parts of the image to examine along with what parts to suppress.
				The way in which biological vision works is still largely unknown which makes it hard to emulate on computers \cite[p. xi]{book:multiViewGeo}.				
				Computer vision systems are still relatively naive, all they "see" is a grid of numbers.%from def:cv
				
				%By default there is no built in pattern recognition, or what some might call, intelligence.
				
				Computer vision is a vast field and hard to define.
				For the purpose of this report it will be defined as:
	
				\begin{quote}
					``\textit{The transformation of data from a still or video camera into either a decision or a new representation.
						All transformations are done for achieving some particular goal.}'' \cite[p. 2]{definition:cv}
				\end{quote}
				
				Even though computer vision in terms of comparison to biological vision, still remains an unsolved problem, there have still been many excellent achievements in the field to date.
			\subsection{Uses of Computer Vision}
				One of the most prominent uses of computer vision currently, is in driver less cars.
				In recent years, they have reached a level of sophistication at which they are being approved by governments to be used on public highways \cite{web:driverlessCars}.
				Complex and expensive equipment with the capability to analyse a three-dimensional scene in real-time is required to achieve this.
				%Add the http://velodynelidar.com/lidar/hdlproducts/hdl64e.aspx here, try and find a price to reference "expensive" maybe mention google explicitely.
				Computer vision is also used on manufacturing production lines.
				\citeasnoun{journal:salmon} set out to try and classify salmon fillets, to see if it was possible to determine whether a salmon fillet had been processed using enzymes. %maybe expand on what they actually did experementally here.
				However, computer vision isn't only available to big companies with large amounts of money to spend on research.
				\begin{quote}
				``\textit{Computer vision is a rapidly growing field, partly as a result of both cheaper and more capable cameras, partly because of affordable processing power, and partly because vision algorithms are starting to mature.}''\cite[p. ix]{definition:cv}
				\end{quote}
				One such device that has been made available is the Kinect by Microsoft.
				The computer vision society found that the capabilities of the Kinect could be extended beyond its intentional use for gaming, and at a much lower cost than traditional three dimensional cameras.
				It has been used in areas such as human activity analysis, where it is able to estimate details about the pose of the human subject in its field of vision \cite{kinect:1}.
				It has also been used to for real time odometry whilst attached to a quadcopter, enabling the production of a three dimensional mapping of a physical space \cite{kinect:2}.
				%The line below is dependant on a reference
				A hacking culture has enabled this type of exploitation of technology to take place and is spurring the creators of these technologies to make them more open.
				
			\subsection{OpenCV}
			
			%Introduction to OpenCV here. Overview, not in depth.	
				OpenCV is an open source computer vision library.
				It was designed for computational efficiency with a strong focus on real-time applications.
				One of OpenCV's goals is to provide a simple-to-use computer vision infrastructure that helps people build fairly sophisticated vision applications quickly \cite[p. 1]{definition:cv}.	
				It aimed to advance computer vision research by providing open and already optimised code.
				Providing a basic vision infrastructure with no need to reinvent the wheel.
				It also provided a common infrastructure on which knowledge of computer vision could be shared between developers.
				This makes code more readable and transferable \cite[p. 6]{definition:cv}. 
				This is an overview for now but more specifics will be covered later in the document.
				
		\section{The Leap Motion Controller}
			\subsection{What is the Leap Motion?}
				%cite leapmotion https://developer.leapmotion.com/articles/intro-to-motion-control
				The Leap Motion Controller is a device aimed at providing a Natural User Interface through hand gestures.
				It is made up of two infra red cameras both with a fisheye lense, set up stereoscopically. %INCLUDE IMAGE OF LEAPMOTION HERE
				It provides functionality out of the box that allows a developer to track movements and gestures made by a hand \cite{web:leapGestures}.
				%Don't really need the price...
				It is discrete in size and has an accessible price. %maybe include the price here
				Leap motion have recently released a new version of their SDK, allowing access to more elements of their Leap Motion Controller. 
				Namely, images from the two cameras. 
				This allows the library user to generate a three dimensional representation of a scene, using techniques of which are discussed later in this document. %define disparity
				The source code for the SDK is closed source, therefore it is not known the current methods by which hands and gestures are classified \cite[p. 217]{journal:leapEvaluation}.
				With the access to the images it allows exploration of other, custom methods for classification.

			\subsection{Uses in Computer Vision}
				The Leap Motion Controller has already been used in many innovative ways.
				An example of this is its use in recognising Australian and Arabic sign language \cite{journal:leapSignLanguage,journal:leapSignLanguage2}.
				However, these solutions only make use of the default detection system in the SDK.
				The default detection system is advanced and accurate enough to detect hand movements to this scale but raw image access is required to detect objects other than hands.
				As stated previously, image access has now been given, and so a whole new future of possibilities have been opened.
				Having access to the images allows for their use in computer vision.
				In turn this means that such an accessible and small device can be used in a variety of new ways.
				By creating an open source computer vision library for the leap motion, it might become of use in the field of robotics.
				It's size would make it suitable for much smaller robots.
				%need to talk about its use in areas such as robotic arms to pick up things for packaging etc.
			\subsection{Why use the Leap Motion?}
			
		\section{Current State-of-the-Art}
		\section{Aims} 
		\begin{itemize}
			\item To develop an open source Computer Vision library usable by the leap motion community.
			\item Enable the leap motion to recognise simple three dimensional shapes using Artificial Intelligence.
		\end{itemize}
		\section{Objectives} 
		\begin{itemize}
			\item Research Artificial Intelligence methods for stereoscopic image object recognition.
			\item Research stereoscopic image processing methods.
			\item Design and develop a library that allows for the training and recognition of simple 3D shapes.
			\item Evaluate the accuracy of the recognition algorithm.
			\item Release an open source library.
			\item Produce a report that communicates my findings
		\end{itemize}
		\section{Solution Approach}
		\section{Report Structure}
			\subsection{Chapter \ref{chap:background}}
			\subsection{Chapter \ref{chap:req}}
			\subsection{Chapter \ref{chap:des&imp}}
			\subsection{Chapter \ref{chap:test}}
			\subsection{Chapter \ref{chap:eval}}
			\subsection{Chapter \ref{chap:concl}}
			
		
	%----------------------------------------- Chapter 2 -----------------------------------------%
	\chapter{Background Research}\label{chap:background}
			\section{Stereopsis and Calibration}
			\begin{quote}
				``\textit{Stereo matching is the process of taking two or more images and estimating a 3D model of the scene by finding matching pixels in the images and converting their 2D positions into 3D depths.}''\cite{book:sam}
			\end{quote}
			The idea behind this stems from the way that biological vision works.
			In humans, depth is perceived from the difference between the images produced by the left and the right eye.
			%cite some stuff about algorithm performance from the scelzki2002aa paper
			To emulate this in computer vision, the images of a scene produced by the cameras are taken from two different positions.
			One of the difficulties then faced with stereo matching is being able to identify the corresponding pixels in each image taken.
			When using two cameras, as those that are to be provided by the Leap Motion, \citeasnoun[p. 415]{definition:cv} define four steps that are needed to achieve successful correspondence between the images produced by each.
			
			\begin{enumerate}
				%CHECK THESE ARE CORRECT
				\item Mathematically remove radial and tangential lense distortion; this is called undistortion. The outputs to this step are undistorted images.
				
				\item Adjust for the angles and distances between cameras, a process called rectification. The outputs of this step are images that are row-aligned and rectified.
				
				\item Find the same features in the left and right camera views, a process known as correspondence. The output of this step is a disparity map, where the disparities are the differences in $x$-coordinates on the image planes of the same feature viewed in the left and right cameras: $x^{l} - x^{r}$.
				
				\item If we know the geometric arrangement of the cameras, then we can turn the disparity map into distances by triangulation. This step is called reprojection, and the output is a depth map.
			\end{enumerate}
			
			For the system to make successful use of the images, it needs to know the set up of the cameras (calibration).
			%epipolar geometry
			Much research has already been done on this.
			As a result, computer vision libraries have appeared, such as OpenCV.
			Libraries like this create a layer of abstraction between the user and the underlying low-level image processing.
			Allowing the process of calibration to be carried out with ease, not needing to know all of the mathematics behind the process.
			Although it is good to have a general overview of what is happening.
			As this is a fundamental element to this project, each stage of calibration will be covered in a little more detail.
				\subsection{Image Correction}
				%get the fundamental matrix using opencv
				Images produced by cameras with lenses suffer from aberrations.
				No lens is perfect.
				Therefore correction is needed to get a true representation of the objects in the image.
				There are many different types of aberration, but the only one that will be discussed here is distortion.
				Distortion is an effect that changes the overall shape of an image (geometric warping) (See Figure \ref{fig:distortion}).
				\begin{figure}[!h]
				\begin{center}
					
    					\includegraphics[scale=0.5]{distortion_1}
    					\caption{Example of radial distortion of a square being corrected \protect\cite{book:multiViewGeo} {\label{fig:distortion}}}
    				\end{center}
				\end{figure}				
				It is caused by the fact that different areas of a lens have slightly different focal lengths \cite[p. 42]{book:modern}.
				There is a lot of information available on how distortion happens so only the basics will be covered here, however some reading suggestions are (READING SUGGESTIONS HERE).
				The fundamental stages of processing images for use in computer vision comes down to geometry.
				To reliably calculate depth in a scene, the system needs to know the intrinsic %and extrinsic 
				parameters of a camera.
				Essentially, the system wants to know if what the camera is producing is a true representation of the scene it is recording.
				Much like when you make a visit to see an optician. 
				The optician will carry out a process to try and discover if what you see is a distortion of reality.
				The parameters will allow the system to take the images and pre process them (undistort) so that it can be confident they are true.
				This is done by producing both a camera geometry and a lens distortion model through a process of calibration.  
				The two models will now be discussed in a little more depth.				
				%more depth here
				\subsection{Camera Model}
				Most of the information available on camera models are focused around a pinhole camera.
				The following description and figures of this model is taken from \citeasnoun[p. 371-373]{definition:cv}, If more depth is needed I advise you read the cited material.
				The pinhole camera model is the most basic type of camera, that still holds the fundamental mathematics on how most modern digital cameras work today.
				\begin{figure}[!ht]
				\begin{center}
					\includegraphics[scale=0.5]{pinhole}
					\caption{Pinhole camera model as shown in \protect\citeasnoun[p. 372]{definition:cv} {\label{fig:pinhole}}}
				\end{center}
				\end{figure}
				In a pinhole camera (See Figure \ref{fig:pinhole}) light reflected from the scene/object travels through a pinhole that is made in the pinhole plane.
				This light then gets projected onto the image plane.
				The size of this projected image is proportional to the focal length $f$.
				Where $Z$ is the distance from the pinhole plane to the object, $X$ is the length of the object and $x$ is the objects image on the image plane.
				Therefore we can work out the size of the object using the equation $-x=f\dfrac{X}{Z}$.
				For mathematical simplification \citeasnoun{definition:cv} states that the negative value can be ommited leaving $x=f\dfrac{X}{Z}$.
				This is done by moving the image plane in front of the pinhole plane, and then treating the pinhole as a center of projection (See Figure \ref{fig:pinhole2}).
				This results in an image that is the correct way up. However it would be impossible to make this camera physically, it is simply to make the mathematics less complicated.
				\begin{figure}[!ht]
				\begin{center}
					\includegraphics[scale=0.5]{pinhole2}
					\caption{Pinhole camera model with image plane in front, as shown in \protect\citeasnoun[p. 372]{definition:cv} {\label{fig:pinhole2}}}
				\end{center}
				\end{figure}
				%THIS ASSUMES KNOWLEDGE ON HOW A CAMERA WORKS
				In the real world, the center of projection will not be precisely in the middle of the imaging sensor of a camera.
				Therefore displacement values need to be considered, $c_{x}$ and $c_{y}$.
				The resulting model allows us to pinpoint the point $Q$ with coordinates $(X,Y,Z)$ as a pixel location given as $(x_{screen}, y_{screen})$.
				With the resulting mathematical model of $x_{screen}=f_{x}\left(\dfrac{X}{Z}\right)+c_{x}$, $y_{screen}=f_{y}\left(\dfrac{Y}{Z}\right)+c_{y}$.
				Having a model like this allows us to define the parameters of the camera so that false transformations of light being collected from the physical world can be corrected.
				%MAYBE ADD SOME MORE STUFF ABOUT PIXEL MATCHING HERE
				This is important for methods of image rectification which will be discussed later.
				\subsection{Lens Distortion}
					The camera models discussed assume that cameras produce images in which the straight lines represent the same straight lines that are in the scene.
					However as discussed earlier a lens is never perfect and so many will create a visible curvature in the projections of straight lines in an image.
					Unless this distortion is taken into account, it becomes impossible to create highly accurate photorealistic reconstructions.
					A phenomenon known as radial distortion produces a visible curvature on the straight lines that appear in the image.
					This is where pixels in the image are displaced away or towards the image centre by an amount proportional to their radial distance.
					These types of distortion are known as barrel and pincushion respectively.
					The Leap Motion has a radial distortion, more specifically it is known as a fisheye.
					The fisheye is a distortion where the image that is produced provides an near 180\degree span side-to-side \cite{book:sam}.
					
				\subsection{Image Rectification}
				
				
			\section{Classification}
				\begin{quote}
					\textit{A classifier is a procedure that accepts a set of features and produces a class label for them.}
					\cite[p. 487]{book:modern}
				\end{quote}
				Classifiers are like rules, data gets passed in, in this case a feature in an image, and the rule returns a class label.
				There are three basic ingredients to the recipe of classification.
				\begin{enumerate}
					\item{Find a labelled dataset.}
					\item{Build the features from within the dataset.}
					\item{Train the classifier with the features.}
				\end{enumerate}
				The most difficult ingredient to apply is building the features.
				Some feature constructions are more suited to certain applications than others.
				It is important to extract the features that show the variation between each class, as opposed to extract the features that show the variation within a class.
				As an example, one might want to extract the features of a car and the features of a van.
				The only features that are important are the ones that differentiate between a car and a van.
				The features that differentiate between one car and another variation of a car are not important.
				On the other hand, it is clear to see there might also be a use for differentiating between two different cars in another scenario.
				Showing that extraction of these features is very much application dependant.
				
				\subsection{Extracting image features}
					\subsubsection{Edge detection}
					In many cases, the first stage of image analysis is to determine where the edges are.
					Evidence suggests that edge detection is an important part of biological vision, particularly in mammals.
					In most situations a predator (or prey) can be seen to be contrasted sharply with its background.
					By noting the edges enables the animal to quickly recognise other animals near it.
					Which explains why camouflage is such a popular technique in the animal kingdom \cite{book:aiIlluminated}.
					An object is separated from its background in an image by an occluding contour.
					An occluding contour is a line where, on one side, the pixels in the image lie on the background, and on the other side, lie on the object.
					Occluding contours form the outlines of objects.
					Edges can be obtained from large image gradients.
					Such as those cause by sharp changes in brightness.
					\subsubsection{Corner detection}
					The terms "feature", "corner" and "image point" are used interchangeably throughout literature.
					For example \citeasnoun[p. 317]{definition:cv} describes corners as
					\begin{quote}
						``\textit{the points that contain enough information to be picked out from one frame to the next.}''
					\end{quote}
					Where as, \citeasnoun[p. 179]{book:modern} describes corners as a local area where the orientation of the image gradient changes sharply; a corner in the more literal sense of the word.
					\citeasnoun{book:modern} says a corner is a more specific term that can be described by the more general term of image point.
					This report will continue using the term corner as \citeasnoun{definition:cv} has, so as to avoid confusion with any code references in OpenCV later on.
					
					Corners are easy to match between different images.
					This makes them extremely useful when reconstructing points in three dimensions when using multiple images.
					Such as the images produced in stereopsis.
					The idea is to extract unique points in an image that can be tracked in another image of the same object or scene.
					Imagine picking a point on a large blank wall in one image, it wont be very easy to find the exact same point in the second image.
					Instead unique points should be selected.
					Such as a tap sticking out of the wall.
					
					
			\section{Vision Libraries}
				\subsection{OpenCV}
				OpenCV is an extensive Computer Vision library that has grown out of a research project run by Intel.
				Universities were seen to be sharing internal computer vision infrastructures between students, and so OpenCV was conceived to make this type of infrastructure universally available.
				Its first alpha release was made in January 1999.
				By enabling computer vision applications, Intel were increasing the need for faster processors, generating more income \cite{definition:cv}.
				Since its first release the library has become well established in the Computer Vision field.
				A simple search with the term OpenCV on google scholar, returns about 38,700 results.
				With the \citeasnoun{definition:cv} book itself cited by 3409 others.
				
				OpenCV is licensed under the BSD open source licence.
				Meaning it is open and free.
				The code can be embedded in other applications for research or for profit and there is no obligation that the created applications code needs to be open or free.
				
				
				\subsection{Point Cloud Library}
				
	%----------------------------------------- Chapter 3 -----------------------------------------%
	\chapter{Requirements Analysis}\label{chap:req}
	This chapter attempts to lay out a set of requirements for the Library, from the point of view of the stakeholders.
	They will set out a contract between the stakeholders and the library developer as a means to show what the library is going to be. It should \textbf{not} show how it will be done - that is the purpose of design \cite{book:dawson}.
	They provide goals upon which a realistic scope for the project may be set and will also be the backbone of the tests that are to be written for \autoref{chap:test}.
	The elements needed to produce a good requirements analysis are as follows.
	\begin{itemize}
		\item \textbf{Stakeholder identification} is important so that the requirements are of use to who they are being developed for.
		\item \textbf{Use case analysis} identifies interactions between your system and users or other external systems. Also helpful in mapping requirements to your systems \cite{book:uml}.
		\item \textbf{Functional requirements} of a system define the system, the data and the user interface \cite{book:dawson}.
		\item \textbf{Non-functional requirements} should lay out constraints, standards, limitations, performance requirements, verification requirements and validation criteria~\cite{book:dawson}.
		\item \textbf{MoSCoW Prioritisation} will set the prioritisation for each requirement. This gives a good identification as to which ones should be developed first.
	\end{itemize}
	The next few sections will elaborate on these and show the documentation produced. 
		
		\section{Stakeholder identification}
			This project is aimed at producing an open source library that will suit a variety of users.
			With the main focus being on the \textit{application developer}.
			Where the \textit{application developer} will use the library to develop other applications.
			The library being developed here will not be an application that can be used as an end user.
			It will be designed to be a simple to use interface that saves the \textit{application developer} a large amount of time when applying it to their own application.
			However, it is possible that other applications might need access to information that is produced by the library, therefore another actor of \textit{user} needs to be considered.
			Through the analysis of existing libraries and solutions that work with other imaging systems, it is possible to identify what would be suitable to provide as a simpler solution for the leap motion.
			The stakeholder also plays a major role in discovering and keeping a project, within a defined scope.
			As the \textit{application developer} will be the main user of the system, they will	have most of the interactions.
		
		\section{Use case analysis}
			\begin{figure}[ht]
			\begin{center}
    			\includegraphics[scale=0.45]{use_case_1}
    			\caption{UML use case diagram \protect {\label{fig:use_case_1}}}
    		\end{center}
			\end{figure}	
			This section has been produced by following the methods outlined by \citeasnoun{book:uml} in ``\textit{Learning UML 2.0}''.
			A use case diagram is used to capture pieces of functionality that a system will provide to the stakeholder and other users.
			Figure~\ref{fig:use_case_1} is an example of a UML use case diagram.
			The stick figures represent actors on the system, the ovals represent use cases and the box enclosing the use cases show the overall system. 
			The actors are acting upon the system through the means of a use case.
			Each connecting line shows the use case upon which the actor should be able to interact.
			In the case of a line between use cases that shows the ``\textit{extend}'' relationship, it shows that the use case might want to completely reuse the connected use cases behaviour.
			However in \textit{UML 2.0} this reuse is optional and dependent on a runtime or system implementation decision.
			Remember at this stage we should not be stating \textbf{how} it should be implemented.
			\input{retrieve_camera_images_use_case}
			\input{manipulate_by_transform_use_case}
			\input{classify_object_use_case}
		\clearpage
		\section{Functional requirements}
			
			As this is a library, user interface requirements do not need to be considered.
			\begin{description}
				\item[Automatic camera configuration.] The library should abstract the leap motion configuration away from the application developer and should not require any explicit calibration.
				\item[Default configuration.] The library should have a default configuration for training and classifying three dimensional shapes.
			\end{description}
				
			
		\section{Non-functional requirements}
			\begin{description}
				\item[Speed.] The library needs to be able to process images and return a useful classification output in near real-time. 
				A minimum of 14 frames per second should be the target.
				\item[Accuracy.] The library should have equal to or greater than 90\% accuracy in its classification ability.
				\item[Multi-platform.] The library should be usable on all of the platforms that the leap motion is currently supported on.
				\item[Java standards] The library should conform to Java standards and have a well documented interface.
				\item[Semantics.] The library should not stray too far from terminologies and semantics used within existing computer vision libraries, unless it is essential. Ensuring users with prior experience of computer vision can use it with ease.
				\item[Extensibility.] As the project will be made open source, the library needs to be designed with extensibility in mind.
			\end{description}
			
		\section{MoSCoW prioritisation}
			MoSCoW is a recognised method for prioritising requirements by their importance. \citeasnoun{book:moscow} defines the categories in order of highest to lowest importance as
			\begin{description}
				\item[Must have. ] All features categorised in this group must me implemented for the system to work.
				\item[Should have. ] Features categorised in this group are of high importance but are not essential to the system working.
				\item[Could have. ] These features will enhance the system by giving it greater functionality, but they do not have to be delivered as a priority.
				\item[Want to have. ] These features only apply to a small group of users and have limited business value.
			\end{description}
			The terminology here has a business system feel.
			Where the term \textit{business value} is used above, in terms of this project it will just mean a limited value to the libraries end result.
			Table~\ref{tab:moscow_table} shows the user stories with their recognised priority.
			\input{moscow_table}
			\clearpage
			The prioritisations have been worked out so that the fundamental elements of the library will get created first.
			Although the end product should be able to classify images, there is not yet a guarantee that it is possible to do so.
			All of the previous requirements have to be implemented before any type of classification can be considered.

    		
	
	%----------------------------------------- Chapter 4 -----------------------------------------%
	\chapter{Design and Implementation}\label{chap:des&imp}
	%----------------------------------------- Chapter 5 -----------------------------------------%
	\section{System Architecture}
	\begin{figure}[ht]
			\begin{center}
    			\includegraphics[scale=0.5]{system_architecture_1}
    			\caption{High level system architecture \protect {\label{fig:system_arch_1}}}
    		\end{center}
			\end{figure}	
	\section{Prototype 1}\label{sec:p1}
		\subsection{Design}
		\subsection{Implementation}
	\section{Prototype 2}\label{sec:p2}
		\subsection{Design}
		\subsection{Implementation}
	\section{Prototype 3}\label{sec:p3}
		\subsection{Design}
		\subsection{Implementation}
	%----------------------------------------- Chapter 6 -----------------------------------------%
	\chapter{Test}\label{chap:test}
		%----------------------------------------- Chapter 7 -----------------------------------------%
	
	\chapter{Evaluation}\label{chap:eval}
	%----------------------------------------- Chapter 7 -----------------------------------------%
	
	\chapter{Conclusion}\label{chap:concl}

	\bibliographystyle{dcu}
	\bibliography{bibfile}
\end{document}
