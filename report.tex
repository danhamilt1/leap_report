%\RequirePackage[l2tabu, orthodox]{nag}

\documentclass[11pt,oneside]{report}
\usepackage[a4paper, margin=1.5in]{geometry}
%\usepackage{fullpage}
\usepackage{url}
\usepackage{graphicx}
\usepackage{harvard}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{bashful}
\usepackage{microtype}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage[font={small,it}]{caption}

\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{\leftmark}
\cfoot{}

\citationmode{abbr}
\renewcommand{\baselinestretch}{1.5}

\newcommand{\varusecase}{\textbf{Use case}}
\newcommand{\vardescription}{Description}
\newcommand{\varactor}{Actor}
\newcommand{\varentry}{Entry Condition}
\newcommand{\varflow}{Flow of Events}
\newcommand{\varaltflow}{Alternative Flow}
\newcommand{\varexit}{Exit Condition}

%\renewcommand{\familydefault}{\sfdefault}


\graphicspath{{images/}}

\title{An Open Source Computer Vision Framework for Leap Motion}
\author{Daniel Hamilton 10026535,\\Computer Science,\\University of the West of England.}
	
\bash
texcount -merge -sum -1 -opt=TCOpt.txt report.tex
\END

\begin{document}
	\maketitle
	\tableofcontents
	\listoffigures
	\listoftables
	
	Word Count: \bashStdout.
	\begin{abstract}
	
	\end{abstract}	
	%----------------------------------------- Chapter 1 -----------------------------------------%
	\chapter{Introduction}\label{chap:introduction}
		\section{Computer Vision: An Intellectual Frontier}
			\subsection{What is Computer Vision?}		
				
				It is very hard for humans to know what their own biological vision really entails, and how difficult it is to reproduce on a computer.
				Information from the eyes is divided into multiple channels, each streaming different kinds of information to the brain.
				The brain then subconsciously groups and identifies the parts of the image to examine along with what parts to suppress.
				The way in which biological vision works is still largely unknown which makes it hard to emulate on computers \cite[p. xi]{book:multiViewGeo}.				
				Computer vision systems are still relatively naive, all they "see" is a grid of numbers.%from def:cv
				
				%By default there is no built in pattern recognition, or what some might call, intelligence.
				
				Computer vision is a vast field and hard to define.
				For the purpose of this report it will be defined as:
	
				\begin{quote}
					``\textit{The transformation of data from a still or video camera into either a decision or a new representation.
						All transformations are done for achieving some particular goal.}'' \cite[p. 2]{definition:cv}
				\end{quote}
				
				Even though computer vision in terms of comparison to biological vision, still remains an unsolved problem, there have still been many excellent achievements in the field to date.
			\subsection{Uses of Computer Vision}
				One of the most prominent uses of computer vision currently, is in driver less cars.
				In recent years, they have reached a level of sophistication at which they are being approved by governments to be used on public highways \cite{web:driverlessCars}.
				Complex and expensive equipment with the capability to analyse a three-dimensional scene in real-time is usually required to achieve this.
				%Add the http://velodynelidar.com/lidar/hdlproducts/hdl64e.aspx here, try and find a price to reference "expensive" maybe mention google explicitely.
				Computer vision is also used on production lines.
				\citeasnoun{journal:salmon} set out to try and classify salmon fillets, to see if it was possible to determine whether a salmon fillet had been processed using enzymes. %maybe expand on what they actually did experementally here.
				However, computer vision isn't only available to big companies with large amounts of money to spend on research.
				\begin{quote}
				``\textit{Computer vision is a rapidly growing field, partly as a result of both cheaper and more capable cameras, partly because of affordable processing power, and partly because vision algorithms are starting to mature.}''\cite[p. ix]{definition:cv}
				\end{quote}
				One such device that has been made available is the Kinect by Microsoft.
				The computer vision society found that the capabilities of the Kinect could be extended beyond its intentional use for gaming, and at a much lower cost than traditional three dimensional cameras.
				It has been used in areas such as human activity analysis, where it is able to estimate details about the pose of the human subject in its field of vision \cite{kinect:1}.
				It has also been used to for real time odometry whilst attached to a quadcopter, enabling the production of a three dimensional mapping of a physical space \cite{kinect:2}.
				%The line below is dependant on a reference
				A hacking culture has enabled this type of exploitation of technology to take place and is spurring the creators of these technologies to make them more open.
				
		\section{The Leap Motion Controller}
			\subsection{What is the Leap Motion?}
				%cite leapmotion https://developer.leapmotion.com/articles/intro-to-motion-control
				The Leap Motion Controller is a device aimed at providing a Natural User Interface through hand gestures.
				It is made up of two infra red cameras both with a fisheye lense, set up stereoscopically. %INCLUDE IMAGE OF LEAPMOTION HERE
				It provides functionality out of the box that allows a developer to track movements and gestures made by a hand \cite{web:leapGestures}.
				%Don't really need the price...
				Leap motion have recently released a new version of their SDK, allowing access to more elements of their Leap Motion Controller. 
				Namely, images from the two cameras. 
				This could possibly be exploited in computer vision. %define disparity
				The  SDK is closed source, therefore it is not known the current methods by which hands and gestures are classified \cite[p. 217]{journal:leapEvaluation}.
				With the access to the images it allows exploration of other, custom methods for object classification and tracking.
				The SDK is for C++, C\#, Objective-C, Java, Python and Javascript.
				It works on Microsoft Windows, Mac OS X and a variety of Linux systems.
				There are currently iOS and Android libraries that are available through request.
				It is therefore a truly multi platform device.
				
				The leap motion is an extremely accessible device and it can be purchased for a price in the range of \pounds55\footnote{\url{http://www.amazon.co.uk/Leap-Motion-Controller-Interacts-Airspace/dp/B00C66Z9ZC}}.
				It is small in size with dimensions 80mm $\times$ 30mm $\times$ 11.25mm.
				
				
				

			\subsection{Uses in Computer Vision}
				The Leap Motion Controller has already been used in many innovative ways.
				An example of this is its use in recognising Australian and Arabic sign language \cite{journal:leapSignLanguage,journal:leapSignLanguage2}.
				However, these solutions only make use of the default detection system in the SDK.
				The default detection system is advanced and accurate enough to detect hand movements to this scale but raw image access is required to detect objects other than hands.
				As stated previously, image access has now been given, and so a whole new future of possibilities have been opened.
				Having access to the images allows for their use in computer vision.
				In turn this means that such an accessible and small device can be used in a variety of new ways.
				By creating an open source computer vision framework for the leap motion, it might become of use in the field of robotics.
				For example, it's size would make it suitable to attach to a robot as a rudimentary depth sensing system for collision avoidance.
				
				%need to talk about its use in areas such as robotic arms to pick up things for packaging etc.
			
		\section{Current State-of-the-Art}
		
			\subsection{OpenCV}
				OpenCV is an extensive Computer Vision framework that has grown out of a research project, initially run by Intel.
				Universities were seen to be sharing internal computer vision infrastructures between students, and so OpenCV was conceived to make this type of infrastructure universally available.
				Its first alpha release was made available in January 1999.
				One of its goals is to provide a simple-to-use computer vision infrastructure that helps people build fairly sophisticated vision applications quickly \cite[p. 1]{definition:cv}.	
				By making computer vision more accessible, Intel were increasing the need for faster processors, in turn generating themselves more income \cite{definition:cv}.
				
				Computer vision is computationally expensive.
				However, many scenarios that use computer vision, need to happen in real time.
				OpenCV was designed for computational efficiency with a strong focus on real-time applications.
				Early on in its life, Intel had pushed for the requirement of more power from CPUs.
				Then in 2010 GPU acceleration was added to OpenCV.
				It has been developed so that adoption from the CPU library to the GPU library is easy, so that developers do not require any training.
				\citeasnoun{journal:nvidia} have shown this extension has sped up algorithms up to four times on a GPU than a CPU.
				
				While the original library is written in C/C++, it has also been wrapped to work on iOS and Java/Android platforms.
				With smartphone processors often improving in each new release, as well as built in cameras, the demand for computer vision applications is increasing.
				Currently, devices are capable of stitching together several photographs into a high resolution panoramic image in real time.
				As well as having the capability of using facial recognition to unlock the device \cite{journal:face}.
				
				Since its first release the library has become well established in the Computer Vision field.
				A simple search with the term ``\textit{OpenCV}'' on google scholar, returns about 38,700 results.
				With the \citeasnoun{definition:cv} book itself cited by 3409 others.
				
				OpenCV is licensed under the BSD open source licence.
				Meaning it is open and free.
				The code can be embedded in other applications for research or for profit and there is no obligation that the created applications code needs to be open or free.
		\section{The Problem}
				Currently, there is no simple solution that allows a developer to simply plug in a leap motion and start developing for computer vision.
				While a computer vision expert might be able to start programming straight away.
				A non-expert would have to learn a large amount before they start to see any results.	
				
				The main aim is to try and expand the areas on the leap motion can be developed with.
				To try and find out whether it is feasible to say that a leap motion can be used for computer vision in a simple way.
				For example, to detect objects other than hands and tools with only a few function calls.
				
				There is a large community of interest in the leap motion forums, with many different projects using the current SDK.
				It might be assumed from the mere interest on threads that contain any form of reference to computer vision, that carrying out research on this will be a useful piece of work.
		\section{Hypotheses}
				Is it possible to use the leap motion in the field of computer vision, so that application developers can exploit the technology in different ways as to what was originally intended?		
				Can this ability then be structured into a framework, so that the effort application developers have to put in to use the leap motion as a solution, is low, making it an option to consider in new projects?	
		\section{Solution Approach}	
			It is envisaged that the framework will be made possible by taking advantage of functionality of the OpenCV library.
			Then exploiting the new image functionality of the leap motion SDK as mentioned earlier.
			The next few sections will outline the processes that will be followed to keep the project within a realistic scope and enable its completion.
					
			\subsection{Project Goals}
				There are two main goals for this project. 
				The primary goal is to develop a computer vision development framework for the leap motion. 
				The secondary goal is to make use of this library in an example application that shows successful detection of objects by the leap motion.
			
			\subsection{Development Methodology}
					There are lots of development methodologies out there.
					It is important to follow a methodology to
					\begin{itemize}
						\item Divide a large problem into easy to understand tasks.
						\item Sharpen focus
						\item Improve time estimates
						\item Provide progress visibility
						\item Provide better structure
						\item Lead to better coding and documentation
					\end{itemize}\cite{book:dawson}.
					
					As this is not a team based project, methodologies such as SCRUM will not be considered.
					Neither will a conventional waterfall model be considered, as this project has a relatively high uncertainty in user requirements.
					Many requirements will be explored throughout the development process.
					Therefore an iterative approach will be followed.
					With the first iteration being an example of a throw away prototype so as to explore technical uncertainties and ascertain initial requirements and scope estimate.
					The later iterations will then follow a more of an evolutionary prototype approach.
					Figure \ref{fig:iterative} gives a visual representation of the iterative approach.
					\begin{figure}[!h]
						\begin{center}
					
    						\includegraphics[scale=0.5]{iterative_development}
    						\caption{Overview of an iterative development methodology {\label{fig:iterative}}}
    					\end{center}
					\end{figure}
			\subsection{Planned Iterations}
					During each iteration a test bench application will be developed that will make use of the framework.
					The development of this prototype will depend upon what level to which the framework is completed.
					Throughout each iteration sufficient unit tests will be implemented based on the requirements.
					Below is an outline of what work is aimed to be completed in each iteration.
					The work breaks down three discrete sections
					\begin{enumerate}
						\item Image processing
						\item Object detection
						\item Classification
					\end{enumerate}
					Each section in the order of the list, is dependant on the previous work being completed.	
					Therefore each iteration will aim to solve
					Each iteration will be fully documented in chapter~\ref{chap:des&imp}.
					\subsubsection{Iteration 1}
					The first iteration will be an initial throwaway prototype.
					It will be used to explore the technologies that are going to be used in the project.
					Mainly to find out how the extensive OpenCV library works, and how well it will interface with the leap motion SDK.
					Image processing and stereo image processing will be explored here.
					Once completed it will allow better requirements to be established for the framework.
					\subsubsection{Iteration 2}
					The second iteration will be the first of an evolutionary prototype.
					It will better expand on the knowledge that will be obtained in iteration 1.
					It will implement the foundation upon which the whole framework will be based.
					The initial requirements will be established here so that a high level design can be accomplished.
					It is hoped that most of the image processing and stereo image processing functionality will be implemented in this stage.
					Providing a solid integration of OpenCV and the leap motion SDK.
					\subsubsection{Iteration 3}
					The third iteration will be the second stage of an evolutionary prototype.
					It will implement the more complex elements of the library, taking full advantage of the work that has been completed in iteration 2.
					Object detection will be implemented here.
					Any previous work will be refined based on any requirement changes.
					If successful and time allows then classification will be explored here.
					\subsection{Project timeline}
					Table \ref{tab:project_timeline} gives a rough outline as to when work will get completed.
					\input{project_timeline}
			\subsection{Tools}
			The framework will be written in Java using the Java SDK.
			For the purpose of scope it will only be developed using version 1.8 of the Java SDK.
			It will be developed in and built to be used within the Eclipse IDE (Luna).
			Development will be carried out mainly on Ubuntu 14.04 but also on Mac OS X 10.10 and Windows 8.1 to ensure multi platform capability.
			Due to time constraints the project will only get fully tested on Ubuntu.
			
			For the leap motion, version 2.2.3 of the SDK will be used.
			For OpenCV version 2.4.10 will be used.
			There is a more recent version 3.0 of OpenCV however it is a beta version and so might be unstable.
		\section{Aims} 
		\begin{itemize}
			\item To develop an open source Computer Vision framework usable by the leap motion community.
			\item Enable the leap motion to recognise simple three dimensional shapes.
		\end{itemize}
		\section{Objectives} 
		\begin{itemize}
			\item Research methods for stereoscopic image object recognition.
			\item Research stereoscopic image processing methods.
			\item Design and develop a framework that allows the leap motion to be used for computer vision.
			\item Evaluate the accuracy of object recognition.
			\item Release an open source framework.
			\item Produce a report that communicates my findings
		\end{itemize}
		
		\section{Report Structure}
			\subsection{Chapter \ref{chap:background}}
			\subsection{Chapter \ref{chap:req}}
			\subsection{Chapter \ref{chap:des&imp}}
			\subsection{Chapter \ref{chap:test}}
			\subsection{Chapter \ref{chap:eval}}
			\subsection{Chapter \ref{chap:concl}}
			
		
	%----------------------------------------- Chapter 2 -----------------------------------------%
	\chapter{Background Research}\label{chap:background}
			\section{Stereopsis and Calibration}
			\begin{quote}
				``\textit{Stereo matching is the process of taking two or more images and estimating a 3D model of the scene by finding matching pixels in the images and converting their 2D positions into 3D depths.}''\cite{book:sam}
			\end{quote}
			The idea behind this stems from the way that biological vision works.
			In humans, depth is perceived from the difference between the images produced by the left and the right eye.
			%cite some stuff about algorithm performance from the scelzki2002aa paper
			To emulate this in computer vision, the images of a scene produced by the cameras are taken from two different positions.
			One of the difficulties then faced with stereo matching is being able to identify the corresponding pixels in each image taken.
			When using two cameras, as those that are to be provided by the Leap Motion, \citeasnoun[p. 415]{definition:cv} define four steps that are needed to achieve successful correspondence between the images produced by each.
			
			\begin{enumerate}
				%CHECK THESE ARE CORRECT
				\item Mathematically remove radial and tangential lense distortion; this is called undistortion. The outputs to this step are undistorted images.
				
				\item Adjust for the angles and distances between cameras, a process called rectification. The outputs of this step are images that are row-aligned and rectified.
				
				\item Find the same features in the left and right camera views, a process known as correspondence. The output of this step is a disparity map, where the disparities are the differences in $x$-coordinates on the image planes of the same feature viewed in the left and right cameras: $x^{l} - x^{r}$.
				
				\item If we know the geometric arrangement of the cameras, then we can turn the disparity map into distances by triangulation. This step is called reprojection, and the output is a depth map.
			\end{enumerate}
			
			For the system to make successful use of the images, it needs to know the set up of the cameras (calibration).
			%epipolar geometry
			Much research has already been done on this.
			As a result, computer vision libraries have appeared, such as OpenCV.
			Libraries like this create a layer of abstraction between the user and the underlying low-level image processing.
			Allowing the process of calibration to be carried out with ease, not needing to know all of the mathematics behind the process.
			Although it is good to have a general overview of what is happening.
			As this is a fundamental element to this project, each stage of calibration will be covered in a little more detail.
				\subsection{Image Correction}
				%get the fundamental matrix using opencv
				Images produced by cameras with lenses suffer from aberrations.
				No lens is perfect.
				Therefore correction is needed to get a true representation of the objects in the image.
				There are many different types of aberration, but the only one that will be discussed here is distortion.
				Distortion is an effect that changes the overall shape of an image (geometric warping) (See Figure \ref{fig:distortion}).
				\begin{figure}[!h]
				\begin{center}
					
    					\includegraphics[scale=0.5]{distortion_1}
    					\caption{Example of radial distortion of a square being corrected \protect\cite{book:multiViewGeo} {\label{fig:distortion}}}
    				\end{center}
				\end{figure}				
				It is caused by the fact that different areas of a lens have slightly different focal lengths \cite[p. 42]{book:modern}.
				There is a lot of information available on how distortion happens so only the basics will be covered here, however some reading suggestions are (READING SUGGESTIONS HERE).
				The fundamental stages of processing images for use in computer vision comes down to geometry.
				To reliably calculate depth in a scene, the system needs to know the intrinsic %and extrinsic 
				parameters of a camera.
				Essentially, the system wants to know if what the camera is producing is a true representation of the scene it is recording.
				Much like when you make a visit to see an optician. 
				The optician will carry out a process to try and discover if what you see is a distortion of reality.
				The parameters will allow the system to take the images and pre process them (undistort) so that it can be confident they are true.
				This is done by producing both a camera geometry and a lens distortion model through a process of calibration.  
				The two models will now be discussed in a little more depth.				
				%more depth here
				\subsection{Camera Model}
				Most of the information available on camera models are focused around a pinhole camera.
				The following description and figures of this model is taken from \citeasnoun[p. 371-373]{definition:cv}, If more depth is needed I advise you read the cited material.
				The pinhole camera model is the most basic type of camera, that still holds the fundamental mathematics on how most modern digital cameras work today.
				\begin{figure}[!ht]
				\begin{center}
					\includegraphics[scale=0.5]{pinhole}
					\caption{Pinhole camera model as shown in \protect\citeasnoun[p. 372]{definition:cv} {\label{fig:pinhole}}}
				\end{center}
				\end{figure}
				In a pinhole camera (See Figure \ref{fig:pinhole}) light reflected from the scene/object travels through a pinhole that is made in the pinhole plane.
				This light then gets projected onto the image plane.
				The size of this projected image is proportional to the focal length $f$.
				Where $Z$ is the distance from the pinhole plane to the object, $X$ is the length of the object and $x$ is the objects image on the image plane.
				Therefore we can work out the size of the object using the equation $-x=f\dfrac{X}{Z}$.
				For mathematical simplification \citeasnoun{definition:cv} states that the negative value can be ommited leaving $x=f\dfrac{X}{Z}$.
				This is done by moving the image plane in front of the pinhole plane, and then treating the pinhole as a center of projection (See Figure \ref{fig:pinhole2}).
				This results in an image that is the correct way up. However it would be impossible to make this camera physically, it is simply to make the mathematics less complicated.
				\begin{figure}[!ht]
				\begin{center}
					\includegraphics[scale=0.5]{pinhole2}
					\caption{Pinhole camera model with image plane in front, as shown in \protect\citeasnoun[p. 372]{definition:cv} {\label{fig:pinhole2}}}
				\end{center}
				\end{figure}
				%THIS ASSUMES KNOWLEDGE ON HOW A CAMERA WORKS
				In the real world, the center of projection will not be precisely in the middle of the imaging sensor of a camera.
				Therefore displacement values need to be considered, $c_{x}$ and $c_{y}$.
				The resulting model allows us to pinpoint the point $Q$ with coordinates $(X,Y,Z)$ as a pixel location given as $(x_{screen}, y_{screen})$.
				With the resulting mathematical model of $x_{screen}=f_{x}\left(\dfrac{X}{Z}\right)+c_{x}$, $y_{screen}=f_{y}\left(\dfrac{Y}{Z}\right)+c_{y}$.
				Having a model like this allows us to define the parameters of the camera so that false transformations of light being collected from the physical world can be corrected.
				%MAYBE ADD SOME MORE STUFF ABOUT PIXEL MATCHING HERE
				This is important for methods of image rectification which will be discussed later.
				\subsection{Lens Distortion}
					The camera models discussed assume that cameras produce images in which the straight lines represent the same straight lines that are in the scene.
					However as discussed earlier a lens is never perfect and so many will create a visible curvature in the projections of straight lines in an image.
					Unless this distortion is taken into account, it becomes impossible to create highly accurate photorealistic reconstructions.
					A phenomenon known as radial distortion produces a visible curvature on the straight lines that appear in the image.
					This is where pixels in the image are displaced away or towards the image centre by an amount proportional to their radial distance.
					These types of distortion are known as barrel and pincushion respectively.
					The Leap Motion has a radial distortion, more specifically it is known as a fisheye.
					The fisheye is a distortion where the image that is produced provides an near 180\degree span side-to-side \cite{book:sam}.
					
				\subsection{Image Rectification}
				
				
			\section{Classification}
				\begin{quote}
					\textit{A classifier is a procedure that accepts a set of features and produces a class label for them.}
					\cite[p. 487]{book:modern}
				\end{quote}
				Classifiers are like rules, data gets passed in, in this case a feature in an image, and the rule returns a class label.
				There are three basic ingredients to the recipe of classification.
				\begin{enumerate}
					\item{Find a labelled dataset.}
					\item{Build the features from within the dataset.}
					\item{Train the classifier with the features.}
				\end{enumerate}
				The most difficult ingredient to apply is building the features.
				Some feature constructions are more suited to certain applications than others.
				It is important to extract the features that show the variation between each class, as opposed to extract the features that show the variation within a class.
				As an example, one might want to extract the features of a car and the features of a van.
				The only features that are important are the ones that differentiate between a car and a van.
				The features that differentiate between one car and another variation of a car are not important.
				On the other hand, it is clear to see there might also be a use for differentiating between two different cars in another scenario.
				Showing that extraction of these features is very much application dependant.
				
				\subsection{Extracting image features}
					\subsubsection{Edge detection}
					In many cases, the first stage of image analysis is to determine where the edges are.
					Evidence suggests that edge detection is an important part of biological vision, particularly in mammals.
					In most situations a predator (or prey) can be seen to be contrasted sharply with its background.
					By noting the edges enables the animal to quickly recognise other animals near it.
					Which explains why camouflage is such a popular technique in the animal kingdom \cite{book:aiIlluminated}.
					An object is separated from its background in an image by an occluding contour.
					An occluding contour is a line where, on one side, the pixels in the image lie on the background, and on the other side, lie on the object.
					Occluding contours form the outlines of objects.
					Edges can be obtained from large image gradients.
					Such as those cause by sharp changes in brightness.
					\subsubsection{Corner detection}
					The terms "feature", "corner" and "image point" are used interchangeably throughout literature.
					For example \citeasnoun[p. 317]{definition:cv} describes corners as
					\begin{quote}
						``\textit{the points that contain enough information to be picked out from one frame to the next.}''
					\end{quote}
					Where as, \citeasnoun[p. 179]{book:modern} describes corners as a local area where the orientation of the image gradient changes sharply; a corner in the more literal sense of the word.
					\citeasnoun{book:modern} says a corner is a more specific term that can be described by the more general term of image point.
					This report will continue using the term corner as \citeasnoun{definition:cv} has, so as to avoid confusion with any code references in OpenCV later on.
					
					Corners are easy to match between different images.
					This makes them extremely useful when reconstructing points in three dimensions when using multiple images.
					Such as the images produced in stereopsis.
					The idea is to extract unique points in an image that can be tracked in another image of the same object or scene.
					Imagine picking a point on a large blank wall in one image, it wont be very easy to find the exact same point in the second image.
					Instead unique points should be selected.
					Such as a tap sticking out of the wall.
					
				
	%----------------------------------------- Chapter 3 -----------------------------------------%
	\chapter{Requirements Analysis}\label{chap:req}
	This chapter attempts to lay out a set of requirements for the framework, from the point of view of the stakeholders.
	They will set out a contract between the stakeholders and the framework developer as a means to show what the framework is going to be. It should \textbf{not} show how it will be done - that is the purpose of design \cite{book:dawson}.
	They provide goals upon which a realistic scope for the project may be set and will also be the backbone of the tests that are to be written for \autoref{chap:test}.
	The elements needed to produce a good requirements analysis are as follows.
	\begin{itemize}
		\item \textbf{Stakeholder identification} is important so that the requirements are of use to who they are being developed for.
		\item \textbf{Use case analysis} identifies interactions between your system and users or other external systems. Also helpful in mapping requirements to your systems \cite{book:uml}.
		\item \textbf{Functional requirements} of a system define the system, the data and the user interface \cite{book:dawson}.
		\item \textbf{Non-functional requirements} should lay out constraints, standards, limitations, performance requirements, verification requirements and validation criteria~\cite{book:dawson}.
		\item \textbf{MoSCoW Prioritisation} will set the prioritisation for each requirement. This gives a good identification as to which ones should be developed first.
	\end{itemize}
	The next few sections will elaborate on these and show the documentation produced. 
		
		\section{Stakeholder identification}
			This project is aimed at producing an open source framework that will suit a variety of users.
			With the main focus being on the \textit{application developer}.
			Where the \textit{application developer} will use the framework to develop other applications.
			The framework being developed here will not be an application that can be used as an end user.
			It will be designed to be a simple to use interface that saves the \textit{application developer} a large amount of time when applying it to their own application.
			However, it is possible that other applications might need access to information that is produced by the framework, therefore another actor of \textit{user} needs to be considered.
			Through the analysis of existing libraries and solutions that work with other imaging systems, it is possible to identify what would be suitable to provide as a simpler solution for the leap motion.
			The stakeholder also plays a major role in discovering and keeping a project, within a defined scope.
			As the \textit{application developer} will be the main user of the system, they will	have most of the interactions.
		
		\section{Use case analysis}
			\begin{figure}[ht]
			\begin{center}
    			\includegraphics[scale=0.45]{use_case_1}
    			\caption{UML use case diagram \protect {\label{fig:use_case_1}}}
    		\end{center}
			\end{figure}	
			This section has been produced by following the methods outlined by \citeasnoun{book:uml} in ``\textit{Learning UML 2.0}''.
			A use case diagram is used to capture pieces of functionality that a system will provide to the stakeholder and other users.
			Figure~\ref{fig:use_case_1} is an example of a UML use case diagram.
			The stick figures represent actors on the system, the ovals represent use cases and the box enclosing the use cases show the overall system. 
			The actors are acting upon the system through the means of a use case.
			Each connecting line shows the use case upon which the actor should be able to interact.
			In the case of a line between use cases that shows the ``\textit{extend}'' relationship, it shows that the use case might want to completely reuse the connected use cases behaviour.
			However in \textit{UML 2.0} this reuse is optional and dependent on a runtime or system implementation decision.
			Remember at this stage we should not be stating \textbf{how} it should be implemented.
			\input{retrieve_camera_images_use_case}
			\input{manipulate_by_transform_use_case}
			\input{classify_object_use_case}
		\clearpage
		\section{Functional requirements}
			
			As this is a framework, user interface requirements do not need to be considered.
			\begin{description}
				\item[Automatic camera configuration.] The framework should abstract the leap motion configuration away from the application developer and should not require any explicit calibration.
				\item[Default configuration.] The framework should have a default configuration for training and classifying three dimensional shapes.
			\end{description}
				
			
		\section{Non-functional requirements}
			\begin{description}
				\item[Speed.] The framework needs to be able to process images and return a useful classification output in near real-time. 
				A minimum of 14 frames per second should be the target.
				\item[Accuracy.] The framework should have equal to or greater than 90\% accuracy in its classification ability.
				\item[Multi-platform.] The framework should be usable on all of the platforms that the leap motion is currently supported on.
				\item[Java standards] The framework should conform to Java standards and have a well documented interface.
				\item[Semantics.] The framework should not stray too far from terminologies and semantics used within existing computer vision libraries, unless it is essential. Ensuring users with prior experience of computer vision can use it with ease.
				\item[Extensibility.] As the project will be made open source, the framework needs to be designed with extensibility in mind.
			\end{description}
			
		\section{MoSCoW prioritisation}
			MoSCoW is a recognised method for prioritising requirements by their importance. \citeasnoun{book:moscow} defines the categories in order of highest to lowest importance as
			\begin{description}
				\item[Must have. ] All features categorised in this group must me implemented for the system to work.
				\item[Should have. ] Features categorised in this group are of high importance but are not essential to the system working.
				\item[Could have. ] These features will enhance the system by giving it greater functionality, but they do not have to be delivered as a priority.
				\item[Want to have. ] These features only apply to a small group of users and have limited business value.
			\end{description}
			The terminology here has a business system feel.
			Where the term \textit{business value} is used above, in terms of this project it will just mean a limited value to the libraries end result.
			Table~\ref{tab:moscow_table} shows the user stories with their recognised priority.
			\input{moscow_table}
			\clearpage
			The prioritisations have been worked out so that the fundamental elements of the framework will get created first.
			Although the end product should be able to classify images, there is not yet a guarantee that it is possible to do so.
			All of the previous requirements have to be implemented before any type of classification can be considered.

    		
	
	%----------------------------------------- Chapter 4 -----------------------------------------%
	\chapter{Design and Implementation}\label{chap:des&imp}
	%----------------------------------------- Chapter 5 -----------------------------------------%
	\section{System Architecture}
	\begin{figure}[ht]
			\begin{center}
    			\includegraphics[scale=0.5]{system_architecture_1}
    			\caption{High level system architecture \protect {\label{fig:system_arch_1}}}
    		\end{center}
			\end{figure}	
	\section{Prototype 1}\label{sec:p1}
		\subsection{Design}
		\subsection{Implementation}
	\section{Prototype 2}\label{sec:p2}
		\subsection{Design}
		\subsection{Implementation}
	\section{Prototype 3}\label{sec:p3}
		\subsection{Design}
		\subsection{Implementation}
	%----------------------------------------- Chapter 6 -----------------------------------------%
	\chapter{Test}\label{chap:test}
		%----------------------------------------- Chapter 7 -----------------------------------------%
	
	\chapter{Evaluation}\label{chap:eval}
	%----------------------------------------- Chapter 7 -----------------------------------------%
	
	\chapter{Conclusion}\label{chap:concl}

	\bibliographystyle{dcu}
	\bibliography{bibfile}
\end{document}
